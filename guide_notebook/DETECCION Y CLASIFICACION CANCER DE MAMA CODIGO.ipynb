{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificación de tumores de mama a partir del análisis de imágenes con técnicas de aprendizaje profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNNs\n",
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from tensorflow.keras.regularizers import L1\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "# YOLO\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación y transformación de las imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se determinan las rutas de los archivos\n",
    "base_dir = r'C:\\Users\\mcamp\\OneDrive\\Documentos\\TFGDEF2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\Mass_Training'\n",
    "output_dir = Path(base_dir)\n",
    "\n",
    "# Se definen las carpetas en las que serán clasificadas las mamografías\n",
    "main_folders = [\"ROI mask images\", \"full mammogram images\", \"cropped images\"]\n",
    "\n",
    "def create_main_folders():\n",
    "    \"\"\"\n",
    "    Función destinada a la creación de carpetas principales si no existen previamente\n",
    "    \"\"\"\n",
    "    for folder in main_folders:\n",
    "        path = output_dir / folder\n",
    "        if not path.exists():\n",
    "            path.mkdir(parents=True)\n",
    "\n",
    "def resize_with_padding(image, target_size, interpolation=cv2.INTER_AREA):\n",
    "    \"\"\"\n",
    "    Función destinada a la redimensión de las imágenes añadiendo bordes negros\n",
    "    \"\"\"\n",
    "    old_size = image.shape[:2]\n",
    "    ratio = min(target_size[1] / old_size[0], target_size[0] / old_size[1])\n",
    "    new_size = (int(old_size[1] * ratio), int(old_size[0] * ratio))\n",
    "\n",
    "    resized_image = cv2.resize(image, new_size, interpolation=interpolation)\n",
    "\n",
    "    delta_w = target_size[0] - new_size[0]\n",
    "    delta_h = target_size[1] - new_size[1]\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "    # Agrega los bordes\n",
    "    color = [0, 0, 0] # negro\n",
    "    padded_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "def preprocess_image(source_path, target_path, target_size):\n",
    "    \"\"\"\n",
    "    Función destinada al procesamiento y guardado de imágenes.\n",
    "    Lee un archivo DICOM, lo redimensiona añadiendo un borde negro y lo guarda como .png.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extrae los datos de píxeles\n",
    "        dicom_data = pydicom.dcmread(source_path)\n",
    "        image_array = dicom_data.pixel_array\n",
    "\n",
    "        # Normaliza la imagen a 8 bits (0-255) si es necesario\n",
    "        if image_array.max() > 255:\n",
    "            image_array = (image_array / image_array.max() * 255).astype(np.uint8)\n",
    "        else:\n",
    "            image_array = image_array.astype(np.uint8)\n",
    "\n",
    "        # Redimensiona la imagen añadiendo borde negro\n",
    "        resized_image = resize_with_padding(image_array, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Guarda como imagen procesada\n",
    "        cv2.imwrite(target_path, resized_image)\n",
    "        print(f\"Procesada y guardada en: {target_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar {source_path}: {e}\")\n",
    "\n",
    "def reorganize_images():\n",
    "    \"\"\"\n",
    "    Función principal destinada a la reorganización y transformación de imágenes.\n",
    "    \"\"\"\n",
    "    create_main_folders()\n",
    "\n",
    "    # Viaja entre las carpetas del dataset hasta alcanzar los documentos fundamentales para el estudio\n",
    "    for class_folder in os.listdir(base_dir):\n",
    "        class_path = os.path.join(base_dir, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            print(f\"Clase: {class_folder}\")\n",
    "\n",
    "            for first_level_folder in os.listdir(class_path):\n",
    "                first_level_path = os.path.join(class_path, first_level_folder)\n",
    "                if os.path.isdir(first_level_path):\n",
    "                    print(f\"Primer nivel: {first_level_folder}\")\n",
    "\n",
    "                    for second_level_folder in os.listdir(first_level_path):\n",
    "                        second_level_path = os.path.join(first_level_path, second_level_folder)\n",
    "                        if os.path.isdir(second_level_path):\n",
    "                            print(f\"Segundo nivel: {second_level_folder}\")\n",
    "\n",
    "                            for third_level_folder in os.listdir(second_level_path):\n",
    "                                third_level_path = os.path.join(second_level_path, third_level_folder)\n",
    "                                if os.path.isdir(third_level_path):\n",
    "                                    words = third_level_folder.split(\"-\")\n",
    "\n",
    "                                    # Verifica el tipo de carpeta y procesa los documentos\n",
    "                                    if \"ROI mask images\" in words:\n",
    "                                        roi_output_folder = output_dir / \"ROI mask images\" / class_folder\n",
    "                                        cropped_output_folder = output_dir / \"cropped images\" / class_folder\n",
    "                                        target_size_roi = (800, 1350)\n",
    "                                        target_size_cropped = (550, 550)\n",
    "\n",
    "                                        # Crea las carpetas de salida si no existen\n",
    "                                        if not roi_output_folder.exists():\n",
    "                                            roi_output_folder.mkdir(parents=True)\n",
    "                                        if not cropped_output_folder.exists():\n",
    "                                            cropped_output_folder.mkdir(parents=True)\n",
    "\n",
    "                                        documents = sorted(os.listdir(third_level_path))\n",
    "                                        for idx, document in enumerate(documents):\n",
    "                                            source_path = os.path.join(third_level_path, document)\n",
    "                                            if os.path.isfile(source_path):\n",
    "                                                if idx == 0:  # La carpeta asignada a la primera imagen es \"cropped images\"\n",
    "                                                    target_path = cropped_output_folder / f\"{class_folder}_{first_level_folder}_{second_level_folder}_{document.replace('.dcm', '.png')}\"\n",
    "                                                    preprocess_image(source_path, target_path, target_size_cropped)\n",
    "                                                elif idx == 1:  # La carpeta asignada a la primera imagen es \"ROI mask images\"\n",
    "                                                    target_path = roi_output_folder / f\"{class_folder}_{first_level_folder}_{second_level_folder}_{document.replace('.dcm', '.png')}\"\n",
    "                                                    preprocess_image(source_path, target_path, target_size_roi)\n",
    "                                    elif \"full mammogram images\" in words:\n",
    "                                        output_folder = output_dir / \"full mammogram images\" / class_folder\n",
    "                                        target_size = (800, 1350)\n",
    "\n",
    "                                        if not output_folder.exists():\n",
    "                                            output_folder.mkdir(parents=True)\n",
    "\n",
    "                                        for document in os.listdir(third_level_path):\n",
    "                                            source_path = os.path.join(third_level_path, document)\n",
    "                                            if os.path.isfile(source_path):\n",
    "                                                target_path = output_folder / f\"{class_folder}_{first_level_folder}_{second_level_folder}_{document.replace('.dcm', '.png')}\"\n",
    "                                                preprocess_image(source_path, target_path, target_size)\n",
    "                                    elif \"cropped images\" in words:\n",
    "                                        output_folder = output_dir / \"cropped images\" / class_folder\n",
    "                                        target_size = (550, 550)\n",
    "\n",
    "                                        if not output_folder.exists():\n",
    "                                            output_folder.mkdir(parents=True)\n",
    "\n",
    "                                        for document in os.listdir(third_level_path):\n",
    "                                            source_path = os.path.join(third_level_path, document)\n",
    "                                            if os.path.isfile(source_path):\n",
    "                                                target_path = output_folder / f\"{class_folder}_{first_level_folder}_{second_level_folder}_{document.replace('.dcm', '.png')}\"\n",
    "                                                preprocess_image(source_path, target_path, target_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reorganize_images()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de directorios e imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de directorios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_train_dir = r'C:\\Users\\mcamp\\OneDrive\\Documentos\\TFGDEF2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\Mass_Training'\n",
    "mass_test_dir = r'C:\\Users\\mcamp\\OneDrive\\Documentos\\TFGDEF2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\Mass_Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size_full = (800, 1350)\n",
    "target_size_cropped = (550, 550)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Procesamiento de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_mass_structure(base_dir, image_type, target_size):\n",
    "    \"\"\"\n",
    "    Función destinada a la carga de imágenes PNG desde los directorios donde se encuentran los datasets de entrenamiento\n",
    "    y validación.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_dir = os.path.join(base_dir, image_type)\n",
    "    class_names = sorted(os.listdir(class_dir))\n",
    "\n",
    "    for label, class_name in enumerate(class_names):\n",
    "        class_path = os.path.join(class_dir, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for file_name in os.listdir(class_path):\n",
    "                if file_name.endswith(\".png\"):\n",
    "                    img_path = os.path.join(class_path, file_name)\n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            if img.mode != \"L\":\n",
    "                                img = img.convert(\"L\") \n",
    "                            img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "                            image_array = img_to_array(img)\n",
    "                            images.append(image_array)\n",
    "                            labels.append(label)\n",
    "                        print(f\"Imagen {img_path} procesada\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error al procesar la imagen {img_path}: {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels), class_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Carga de imágenes de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_images, full_train_labels, class_names = load_images_from_mass_structure(\n",
    "    mass_train_dir, \"full mammogram images\", target_size=target_size_full\n",
    ")\n",
    "cropped_train_images, cropped_train_labels, _ = load_images_from_mass_structure(\n",
    "    mass_train_dir, \"cropped images\", target_size=target_size_cropped\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Carga de imágenes de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_val_images, full_val_labels, _ = load_images_from_mass_structure(\n",
    "    mass_test_dir, \"full mammogram images\", target_size=target_size_full\n",
    ")\n",
    "cropped_val_images, cropped_val_labels, _ = load_images_from_mass_structure(\n",
    "    mass_test_dir, \"cropped images\", target_size=target_size_cropped\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comprobación y corrección de dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se verifican las dimensiones iniciales\n",
    "print(\"Formas iniciales:\")\n",
    "print(\"Full train:\", full_train_images.shape)\n",
    "print(\"Cropped train:\", cropped_train_images.shape)\n",
    "print(\"Full val:\", full_val_images.shape)\n",
    "print(\"Cropped val:\", cropped_val_images.shape)\n",
    "\n",
    "# Se corrigen las dimensiones si están invertidas\n",
    "full_train_images = np.transpose(full_train_images, (0, 2, 1, 3))\n",
    "cropped_train_images = np.transpose(cropped_train_images, (0, 2, 1, 3))\n",
    "full_val_images = np.transpose(full_val_images, (0, 2, 1, 3))\n",
    "cropped_val_images = np.transpose(cropped_val_images, (0, 2, 1, 3))\n",
    "\n",
    "# Se verifican las dimensiones después de la corrección\n",
    "print(\"Formas corregidas:\")\n",
    "print(\"Full train:\", full_train_images.shape)\n",
    "print(\"Cropped train:\", cropped_train_images.shape)\n",
    "print(\"Full val:\", full_val_images.shape)\n",
    "print(\"Cropped val:\", cropped_val_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model(input_shape_full, input_shape_cropped, num_classes):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción del modelo multimodal empleado en este TFG.\n",
    "\n",
    "    La arquitectura se compone de dos submodelos independientes, uno para las mamografías completas y otro para las \n",
    "    recortadas donde se muestra la localización ampliada del tumor. Ambos submodelos siguen una estructura similar de\n",
    "    capas convolucionales pero funcionan como flujos de procesamiento separados hasta que sus salidas son combinadas\n",
    "    con el fin de optimizar la eficiencia del entrenamiento y reducir la sobrecarga computacional.\n",
    "    \"\"\"\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(concatenated)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"class_output\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multimodal = create_multimodal_model(\n",
    "    input_shape_full=(800, 1350, 1), \n",
    "    input_shape_cropped=(550, 550, 1), \n",
    "    num_classes=len(class_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación\n",
    "history = model_multimodal.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    \"\"\"\n",
    "    Función destinada a la generación de dos gráficas que muestran la evolución del entrenamiento y validación de un\n",
    "    modelo de aprendizaje automático a lo largo de las épocas extrayendo la información del historial de información.\n",
    "    \"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    fig.suptitle('Gráficas de la precisión y la pérdida del entrenamiento y la validación')\n",
    "    for i, (data, label) in enumerate(zip([(acc, val_acc), (loss, val_loss)], [\"Precisión\", \"Pérdida\"])):\n",
    "        ax[i].plot(epochs, data[0], 'r', label=\"Entrenamiento \" + label)\n",
    "        ax[i].plot(epochs, data[1], 'b', label=\"Validación \" + label)\n",
    "        ax[i].legend()\n",
    "        ax[i].set_xlabel('epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización del modelo inicial a partir de la implementación de diversas técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con Aumento de Datos añadido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Especificación de los parámetros empleados en el Aumento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_augmentation():\n",
    "    \"\"\"\n",
    "    Función destinada a la especificación de los parámetros empleados en el Aumento de Datos.\n",
    "    \"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.3),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomTranslation(0.2, 0.2)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_augmentation(input_shape_full, input_shape_cropped, num_classes):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye la técnica denominada como Aumento de Datos\n",
    "    a partir del modelo multimodal anterior.\n",
    "    \"\"\"\n",
    "    data_augmentation = create_data_augmentation()\n",
    "\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = data_augmentation(input_full)  # Se aplica el Aumento De Datos\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = data_augmentation(input_cropped)  # Se aplica el Aumento De Datos\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(concatenated)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"class_output\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_augmentation = create_multimodal_model_with_augmentation(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación\n",
    "history_with_augmentation = model_with_augmentation.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graficación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con Abandono añadido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_dropout(input_shape_full, input_shape_cropped, num_classes):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye la técnica denominada como Abandono\n",
    "    a partir del modelo multimodal anterior sin Aumento de Datos incluido.\n",
    "    \"\"\"\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales con Dropout\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(concatenated)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)  # Dropout\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"class_output\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_dropout_and_aug(input_shape_full, input_shape_cropped, num_classes):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye la técnica denominada como Abandono\n",
    "    a partir del modelo multimodal anterior con Aumento de Datos incluido.\n",
    "    \"\"\"\n",
    "    data_augmentation = create_data_augmentation()\n",
    "\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = data_augmentation(input_full)\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = data_augmentation(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales con Dropout\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(concatenated)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)  # Dropout\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"class_output\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_dropout = create_multimodal_model_with_dropout(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_dropout_and_aug = create_multimodal_model_with_dropout_and_aug(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación sin Aumento de Datos incluido\n",
    "history_with_dropout = model_with_dropout.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación con Aumento de Datos incluido\n",
    "history_with_dropout_and_aug = model_with_dropout_and_aug.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graficación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_dropout_and_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con Normalización por Lotes añadido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_bn(input_shape_full, input_shape_cropped, num_classes):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye la técnica denominada como Normalización\n",
    "    por Lotes a partir del modelo multimodal anterior sin Aumento de Datos incluido.\n",
    "    \"\"\"\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales con Batch Normalization\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(concatenated)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"class_output\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_bn_and_aug(input_shape_full, input_shape_cropped, num_classes):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye la técnica denominada como Normalización\n",
    "    por Lotes a partir del modelo multimodal anterior con Aumento de Datos incluido.\n",
    "    \"\"\"\n",
    "    data_augmentation = create_data_augmentation()\n",
    "\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = data_augmentation(input_full)\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = data_augmentation(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales con Batch Normalization\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(concatenated)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"class_output\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_bn = create_multimodal_model_with_bn(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_bn_and_aug = create_multimodal_model_with_bn_and_aug(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_with_bn = model_with_bn.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_with_bn_and_aug = model_with_bn_and_aug.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graficación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_bn_and_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con Regularización L1 añadido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_l1(input_shape_full, input_shape_cropped, num_classes, l1_value):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye la técnica denominada como Regularización L1\n",
    "    a partir del modelo multimodal anterior sin Aumento de Datos incluido.\n",
    "    \"\"\"\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales con Regularización L1\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=L1(l1=l1_value))(concatenated)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=L1(l1=l1_value))(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_l1_and_aug(input_shape_full, input_shape_cropped, num_classes, l1_value):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye la técnica denominada como Regularización L1\n",
    "    a partir del modelo multimodal anterior con Aumento de Datos incluido.\n",
    "    \"\"\"\n",
    "    data_augmentation = create_data_augmentation()\n",
    "\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = data_augmentation(input_full)\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = data_augmentation(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales con Regularización L1\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=L1(l1=l1_value))(concatenated)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=L1(l1=l1_value))(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_l1 = create_multimodal_model_with_l1(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names),\n",
    "    l1_value=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_l1_and_aug = create_multimodal_model_with_l1_and_aug(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names),\n",
    "    l1_value=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación sin Aumento de Datos incluido\n",
    "history_with_l1 = model_with_l1.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación con Aumento de Datos incluido\n",
    "history_with_l1_and_aug = model_with_l1_and_aug.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graficación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_l1_and_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con Regularización L2 añadido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_l2(input_shape_full, input_shape_cropped, num_classes, l2_value):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye la técnica denominada como Regularización L2\n",
    "    a partir del modelo multimodal anterior sin Aumento de Datos incluido.\n",
    "    \"\"\"\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales con Regularización L2\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=L2(l2=l2_value))(concatenated)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=L2(l2=l2_value))(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_l2_and_aug(input_shape_full, input_shape_cropped, num_classes, l2_value):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye la técnica denominada como Regularización L2\n",
    "    a partir del modelo multimodal anterior con Aumento de Datos incluido.\n",
    "    \"\"\"\n",
    "    data_augmentation = create_data_augmentation()\n",
    "\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = data_augmentation(input_full)\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = data_augmentation(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales con Regularización L2\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=L2(l2=l2_value))(concatenated)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=L2(l2=l2_value))(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_l2 = create_multimodal_model_with_l2(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names),\n",
    "    l2_value=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_l2_and_aug = create_multimodal_model_with_l2_and_aug(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names),\n",
    "    l2_value=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación sin Aumento de Datos incluido\n",
    "history_with_l2 = model_with_l2.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación con Aumento de Datos incluido\n",
    "history_with_l2_and_aug = model_with_l2_and_aug.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graficación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_l2_and_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con Regularización L1 y L2 añadido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_l1l2(input_shape_full, input_shape_cropped, num_classes, l1_value, l2_value):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye las técnicas denominadas como Regularización \n",
    "    L1 y L2 a partir del modelo multimodal anterior sin Aumento de Datos incluido.\n",
    "    \"\"\"\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales con Regularización L1 y L2\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=L1L2(l1=l1_value, l2=l2_value))(concatenated)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=L1L2(l1=l1_value, l2=l2_value))(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_model_with_l1l2_and_aug(input_shape_full, input_shape_cropped, num_classes, l1_value, l2_value):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción de un nuevo modelo multimodal que incluye las técnicas denominadas como Regularización \n",
    "    L1 y L2 a partir del modelo multimodal anterior con Aumento de Datos incluido.\n",
    "    \"\"\"\n",
    "    data_augmentation = create_data_augmentation()\n",
    "\n",
    "    # Submodelo para imágenes completas\n",
    "    input_full = tf.keras.Input(shape=input_shape_full, name=\"full_mammogram_input\")\n",
    "    x_full = data_augmentation(input_full)\n",
    "    x_full = tf.keras.layers.Rescaling(1.0 / 255)(input_full)\n",
    "    x_full = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_full)\n",
    "    x_full = tf.keras.layers.BatchNormalization()(x_full)\n",
    "    x_full = tf.keras.layers.MaxPooling2D((2, 2))(x_full)\n",
    "    x_full = tf.keras.layers.Flatten()(x_full)\n",
    "\n",
    "    # Submodelo para imágenes recortadas\n",
    "    input_cropped = tf.keras.Input(shape=input_shape_cropped, name=\"cropped_image_input\")\n",
    "    x_cropped = data_augmentation(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Rescaling(1.0 / 255)(input_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_cropped)\n",
    "    x_cropped = tf.keras.layers.BatchNormalization()(x_cropped)\n",
    "    x_cropped = tf.keras.layers.MaxPooling2D((2, 2))(x_cropped)\n",
    "    x_cropped = tf.keras.layers.Flatten()(x_cropped)\n",
    "\n",
    "    # Concatenación y capas finales con Regularización L1 y L2\n",
    "    concatenated = tf.keras.layers.Concatenate()([x_full, x_cropped])\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=L1L2(l1=l1_value, l2=l2_value))(concatenated)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation=\"softmax\", kernel_regularizer=L1L2(l1=l1_value, l2=l2_value))(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_full, input_cropped], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_l1l2 = create_multimodal_model_with_l1l2(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names),\n",
    "    l1_value=0.01,\n",
    "    l2_value=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_l1l2_and_aug = create_multimodal_model_with_l1l2_and_aug(\n",
    "    input_shape_full=(800, 1350, 1),\n",
    "    input_shape_cropped=(550, 550, 1),\n",
    "    num_classes=len(class_names),\n",
    "    l1_value=0.01,\n",
    "    l2_value=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del nuevo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación sin Aumento de Datos incluido\n",
    "history_with_l1l2 = model_with_l1l2.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación com Aumento de Datos incluido\n",
    "history_with_l1l2_and_aug = model_with_l1l2_and_aug.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graficación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_l1l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_l1l2_and_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con Detección Temprana añadida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Especificación de la clase que define la Detección Temprana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Esta clase define una callback personalizada de Detección Temprana para Keras. \n",
    "    Su función es detener automáticamente el entrenamiento del modelo si se cumplen las condiciones determinadas en la\n",
    "    función on_epoch_end.\n",
    "    \"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Función destinada a la detección automática del entrenamiento del modelo si se cumplen simultáneamente las\n",
    "        siguientes condiciones:\n",
    "          - La excatitud del entrenamiento es mayor o igual a 0.95.\n",
    "          - La exactitud de la validación es mayor o igual a 0.90.\n",
    "        \"\"\"\n",
    "        if logs[\"accuracy\"] >= 0.95 and logs[\"val_accuracy\"] >= 0.90:\n",
    "            self.model.stop_training = True\n",
    "            print(\"\\nSe alcanzaron los criterios de Detección Temprana.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento con Detección Temprana añadida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación sin Aumento de Datos incluido\n",
    "history_with_early_stopping = model_with_l1l2.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se almacena toda la información del proceso de entrenamiento y validación con Aumento de Datos incluido\n",
    "history_with_early_stopping_and_aug = model_with_l1l2_and_aug.fit(\n",
    "    {\"full_mammogram_input\": full_train_images, \"cropped_image_input\": cropped_train_images},\n",
    "    full_train_labels,\n",
    "    validation_data=(\n",
    "        {\"full_mammogram_input\": full_val_images, \"cropped_image_input\": cropped_val_images},\n",
    "        full_val_labels\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=12,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graficación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_acc(history_with_early_stopping_and_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganización del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se determinan las rutas de los archivos\n",
    "base_dir = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P3\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\Mass_Training\"\n",
    "output_dir = Path(r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\train\")\n",
    "\n",
    "def create_main_folders():\n",
    "    \"\"\"\n",
    "    Función destinada a la confirmación de la existencia de la carpeta de destino.\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def process_image_no_resize(source_path, target_path):\n",
    "    \"\"\"\n",
    "    Función destinada al procesamiento y guardado de la imagen sin recorte ni resize.\n",
    "    Lee un archivo DICOM y lo guarda como .png sin redimensionar.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dicom_data = pydicom.dcmread(source_path)\n",
    "        image_array = dicom_data.pixel_array\n",
    "\n",
    "        # Convierte a 8 bits si fuera necesario\n",
    "        if image_array.max() > 255:\n",
    "            image_array = (image_array / image_array.max() * 255).astype(np.uint8)\n",
    "        else:\n",
    "            image_array = image_array.astype(np.uint8)\n",
    "\n",
    "        # Guarda la imagen en su tamaño original\n",
    "        cv2.imwrite(str(target_path), image_array)\n",
    "        print(f\"Guardada sin redimensionar en: {target_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar {source_path}: {e}\")\n",
    "\n",
    "def reorganize_images():\n",
    "    \"\"\"\n",
    "    Función principal destinada a la reorganización de imágenes con la misma estructura sin redimensionar.\n",
    "    \"\"\"\n",
    "    create_main_folders()\n",
    "\n",
    "    for class_folder in os.listdir(base_dir):\n",
    "        class_path = os.path.join(base_dir, class_folder)\n",
    "        if os.path.isdir(class_path):  # Verifica si es un directorio (clase)\n",
    "            print(f\"Clase: {class_folder}\")\n",
    "\n",
    "            for first_level_folder in os.listdir(class_path):\n",
    "                first_level_path = os.path.join(class_path, first_level_folder)\n",
    "                if os.path.isdir(first_level_path):  # Verifica si es un directorio\n",
    "                    print(f\"Primer nivel: {first_level_folder}\")\n",
    "\n",
    "                    for second_level_folder in os.listdir(first_level_path):\n",
    "                        second_level_path = os.path.join(first_level_path, second_level_folder)\n",
    "                        if os.path.isdir(second_level_path):  # Verifica si es un directorio\n",
    "                            print(f\"Segundo nivel: {second_level_folder}\")\n",
    "\n",
    "                            for third_level_folder in os.listdir(second_level_path):\n",
    "                                third_level_path = os.path.join(second_level_path, third_level_folder)\n",
    "                                if os.path.isdir(third_level_path):  # Verifica si es un directorio\n",
    "                                    words = third_level_folder.split(\"-\")\n",
    "\n",
    "                                    # full mammogram images\n",
    "                                    if \"full mammogram images\" in words:\n",
    "                                        # Procesar las imágenes \"full mammogram images\"\n",
    "                                        for document in os.listdir(third_level_path):\n",
    "                                            source_path = os.path.join(third_level_path, document)\n",
    "                                            if os.path.isfile(source_path):\n",
    "                                                base_name = f\"{first_level_folder}_{second_level_folder}_{third_level_folder}\"\n",
    "                                                target_path = output_dir / f\"{base_name}.png\"\n",
    "                                                process_image_no_resize(source_path, target_path)\n",
    "\n",
    "                                    # ROI mask images\n",
    "                                    elif \"ROI mask images\" in words:\n",
    "                                        # Carpeta de salida para ROI en PNG (sin subcarpeta de clase)\n",
    "                                        roi_output_folder = output_dir / \"ROI mask images\"\n",
    "\n",
    "                                        # Crear las carpetas si no existen\n",
    "                                        roi_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                                        documents = sorted(os.listdir(third_level_path))  # Archivos .dcm, etc.\n",
    "                                        for idx, document in enumerate(documents):\n",
    "                                            source_path = os.path.join(third_level_path, document)\n",
    "                                            if os.path.isfile(source_path):\n",
    "                                                base_name = f\"{first_level_folder}_{second_level_folder}_{third_level_folder}_{idx}\"\n",
    "                                                if idx == 1:\n",
    "                                                    # Guardar la segunda imagen en \"ROI mask images\"\n",
    "                                                    roi_target_path = roi_output_folder / f\"{base_name}.png\"\n",
    "                                                    process_image_no_resize(source_path, roi_target_path)\n",
    "                                                else: pass\n",
    "                                    else: pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reorganize_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_yolo_label_from_roi_png(roi_png_path, label_path, class_id=0):\n",
    "    \"\"\"\n",
    "    Función destinada a la extracción del bounding box de la ROI mask y generación del .txt en formato YOLO dada una imagen.\n",
    "    Procesa una máscara ROI en formato PNG para generar etiquetas YOLO.\n",
    "    Asume que la máscara tiene fondo negro (0) y la ROI en blanco (>127).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mask_array = cv2.imread(str(roi_png_path), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if mask_array is None:\n",
    "            print(f\"No se pudo leer la imagen: {roi_png_path}\")\n",
    "            return\n",
    "\n",
    "        height, width = mask_array.shape\n",
    "\n",
    "        # Binariza\n",
    "        _, thresh = cv2.threshold(mask_array, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Realiza operaciones morfológicas\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "        cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        contours, _ = cv2.findContours(cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours = [c for c in contours if cv2.contourArea(c) > 50]\n",
    "\n",
    "        if not contours:\n",
    "            print(f\"No se encontraron contornos en {roi_png_path.name}\")\n",
    "            return\n",
    "\n",
    "        # Guarda la etiqueta YOLO\n",
    "        with open(label_path, \"w\") as f:\n",
    "            for c in contours:\n",
    "                x, y, w_box, h_box = cv2.boundingRect(c)\n",
    "\n",
    "                x_center = (x + w_box / 2) / width\n",
    "                y_center = (y + h_box / 2) / height\n",
    "                w_norm = w_box / width\n",
    "                h_norm = h_box / height\n",
    "\n",
    "                f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\\n\")\n",
    "\n",
    "        print(f\"Etiqueta YOLO generada: {label_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar {roi_png_path.name}: {e}\")\n",
    "\n",
    "\n",
    "def process_roi_png_folder_in_place(roi_folder_path, class_id=0):\n",
    "    \"\"\"\n",
    "    Función destinada a la extracción del bounding box de la ROI mask y generación del .txt en formato YOLO dada una carpeta.\n",
    "    Recorre las imágenes .png de una carpeta procesando sus máscaras ROI para generar etiquetas YOLO.\n",
    "    \"\"\"\n",
    "    roi_folder = Path(roi_folder_path)\n",
    "    label_folder = roi_folder / \"labels\"\n",
    "    label_folder.mkdir(exist_ok=True)\n",
    "\n",
    "    for roi_png in sorted(roi_folder.glob(\"*.png\")):\n",
    "        base_name = roi_png.stem\n",
    "        label_output_path = label_folder / f\"{base_name}.txt\"\n",
    "\n",
    "        print(f\"Procesando ROI PNG: {roi_png.name}\")\n",
    "        generate_yolo_label_from_roi_png(roi_png, label_output_path, class_id=class_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_folder_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\Mass_Test\\ROI mask images\\TODO\"\n",
    "process_roi_png_folder_in_place(roi_folder_path, class_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupación de los archivos .txt que hacen referencia a la misma mama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping_and_renaming_labels(carpeta_labels, eliminar_originales=True):\n",
    "    \"\"\" \n",
    "    Función destinada, dada una carpeta con archivos .txt, a la agrupación en función de los nombres de los archivos \n",
    "    prescindiendo del número final puesto que hacen referencia a la misma mama.\n",
    "    De esta forma, realiza una de las siguientes acciones para cada grupo:\n",
    "    - Si hay más de un archivo en el grupo, los combina en un solo archivo de salida.\n",
    "    - Si solo hay un archivo en el grupo, lo copia sin cambios a un archivo de salida.\n",
    "    Los archivos originales se eliminan después de ser procesados, si el parámetro 'eliminar_originales' es True.\n",
    "    La función imprime mensajes sobre el progreso de cada acción: combinación, copia o eliminación.\n",
    "    \"\"\"\n",
    "    carpeta = Path(carpeta_labels)\n",
    "    archivos = sorted([f for f in carpeta.glob(\"*.txt\") if f.is_file()])\n",
    "\n",
    "    grupos = defaultdict(list)\n",
    "\n",
    "    for archivo in archivos:\n",
    "        nombre = archivo.stem  # sin .txt\n",
    "        partes = nombre.split(\"_\")\n",
    "        if partes[-1].isdigit():\n",
    "            clave = \"_\".join(partes[:-1])\n",
    "            grupos[clave].append(archivo)\n",
    "\n",
    "    for clave, lista_archivos in grupos.items():\n",
    "        archivo_salida = carpeta / f\"{clave}.txt\"\n",
    "\n",
    "        if len(lista_archivos) > 1:\n",
    "            with open(archivo_salida, \"w\") as salida:\n",
    "                for archivo in sorted(lista_archivos):\n",
    "                    with open(archivo, \"r\") as f:\n",
    "                        salida.writelines(f.readlines())\n",
    "            print(f\"Combinado: {archivo_salida.name} ({len(lista_archivos)} archivos)\")\n",
    "\n",
    "        elif len(lista_archivos) == 1:\n",
    "            shutil.copy(lista_archivos[0], archivo_salida)\n",
    "            print(f\"Copiado: {lista_archivos[0].name} ➜ {archivo_salida.name}\")\n",
    "\n",
    "        if eliminar_originales:\n",
    "            for archivo in lista_archivos:\n",
    "                archivo.unlink()\n",
    "                print(f\"Eliminado: {archivo.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_folder_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\Mass_Test\\ROI mask images\\TODO\\labels\"\n",
    "grouping_and_renaming_labels(label_folder_path, eliminar_originales=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirmación de que cada imagen png posea un archivo txt con su nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_names_between_pngs_and_txts(folder1, folder2):\n",
    "    \"\"\"\n",
    "    Función destinada a la comparación de los nombres de archivos (sin extensión) entre dos carpetas.\n",
    "    Imprime si son iguales o muestra las diferencias.\n",
    "    \"\"\"\n",
    "    folder1 = Path(folder1)\n",
    "    folder2 = Path(folder2)\n",
    "\n",
    "    files1 = {f.stem for f in folder1.glob(\"*\") if f.is_file()}\n",
    "    files2 = {f.stem for f in folder2.glob(\"*\") if f.is_file()}\n",
    "\n",
    "    only_in_1 = files1 - files2\n",
    "    only_in_2 = files2 - files1\n",
    "\n",
    "    if not only_in_1 and not only_in_2:\n",
    "        print(\"Todos los archivos coinciden entre ambas carpetas.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Hay diferencias entre las carpetas:\")\n",
    "        if only_in_1:\n",
    "            print(f\"Solo en {folder1.name}:\")\n",
    "            for f in sorted(only_in_1):\n",
    "                print(f\"  - {f}\")\n",
    "        if only_in_2:\n",
    "            print(f\"Solo en {folder2.name}:\")\n",
    "            for f in sorted(only_in_2):\n",
    "                print(f\"  - {f}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_names_between_pngs_and_txts(\n",
    "    r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\Mass_Training\\ROI mask images\\TODO(UNIDO)\\labels\",\n",
    "    r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\Mass_Training\\full mammogram images\\TODO\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacción y entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo sin Optimizador AdamW ni flip horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo():\n",
    "    \"\"\"\n",
    "    Función destinada al entrenamiento de un modelo YOLOv8 con el fin de localizar con la mayor exactitud posible \n",
    "    la localización de los tumores dado el dataset de mamografías.\n",
    "    \"\"\"\n",
    "    # Carga el modelo base \"yolov8n.pt\"\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    # Entrena durante 100 épocas con imágenes de tamaño 640x640 y un batch de 8\n",
    "    model.train(\n",
    "        data=r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\mammo.yaml\",    \n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=8,\n",
    "        name=\"mammo_yolo\",  # Guarda el experimento bajo el nombre \"mammo_yolo\"\n",
    "        plots=True          # Genera gráficas del entrenamiento.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo con Optimizador AdamW y flip horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_adamw_flip():\n",
    "    \"\"\"\n",
    "    Función destinada al entrenamiento de un modelo YOLOv8 incluyendo el uso del optimizador AdamW y la técnica denominada\n",
    "    como Aumento de Datos con el fin de localizar con la mayor exactitud posible la localización de los tumores dado\n",
    "    el dataset de mamografías.\n",
    "    \"\"\"\n",
    "    # Carga el modelo base \"yolov8n.pt\" y lo entrena usando la configuración definida en el archivo YAML\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    # Entrena durante 100 épocas con imágenes de tamaño 640x640 y un batch de 8\n",
    "    model.train(\n",
    "        data=r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\mammo.yaml\",    \n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=8,\n",
    "        name=\"mammo_yolo_adamw_flip\",\n",
    "        augment=True,    # Habilita el augmentation\n",
    "        fliplr=0.5,      # Probabilidad de flip horizontal = 50%\n",
    "        flipud=0.0,      # Probabilidad de flip vertical = 0% (desactivado)\n",
    "        optimizer=\"AdamW\",\n",
    "        plots=True \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_adamw_flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación gráfica del rendimiento de ambos modelos durante el entrenamiento y la validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "results_png_path = os.path.join(\"runs\\detect\\mammo_yolo_adamw_flip7\", \"results.png\")\n",
    "img = plt.imread(results_png_path)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "results_png_path = os.path.join(\"runs\\detect\\mammo_yolo_flip\", \"results.png\")\n",
    "img = plt.imread(results_png_path)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de las etiquetas originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box_yolo_style(\n",
    "    img, \n",
    "    x1, y1, x2, y2, \n",
    "    label=\"tumor\", \n",
    "    score=0.0, \n",
    "    font_scale=0.8, \n",
    "    font_thickness=2,\n",
    "    color=(0, 0, 255),\n",
    "    min_box_size=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Función destinada a la creacción sobre la imagen de una caja basándose en el estilo de YOLO, es decir, un rectángulo rojo \n",
    "    señalando la zona afectada con un pequeño recuadro encima de él que continene la etiqueta que indica la puntuación sobre \n",
    "    cuánta exactitud posee la posición del rectángulo predicho por el modelo respecto a la posición real de éste.\n",
    "    \"\"\"\n",
    "\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "    # Dibuja el rectángulo principal\n",
    "    box_w = x2 - x1\n",
    "    box_h = y2 - y1\n",
    "    if box_w < min_box_size or box_h < min_box_size:\n",
    "        thickness = 3\n",
    "    else:\n",
    "        thickness = 2\n",
    "\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    # Crea el texto calculando además su tamaño\n",
    "    text = f\"{label} {score:.2f}\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    (text_w, text_h), baseline = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "\n",
    "    # Posiciona la etiqueta\n",
    "    text_x = x1\n",
    "    text_y = y1 - 5\n",
    "\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    top_rect_y = text_y - text_h - baseline\n",
    "    if top_rect_y < 0:\n",
    "        text_y = y1 + text_h + baseline + 5\n",
    "        top_rect_y = y1\n",
    "\n",
    "    # Dibuja el recuadro relleno para el texto\n",
    "    cv2.rectangle(\n",
    "        img,\n",
    "        (text_x, top_rect_y),\n",
    "        (text_x + text_w, text_y),\n",
    "        color,\n",
    "        -1\n",
    "    )\n",
    "\n",
    "    # Posiciona el texto en blanco encima\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text,\n",
    "        (text_x, text_y - baseline),\n",
    "        font,\n",
    "        font_scale,\n",
    "        (255, 255, 255),\n",
    "        font_thickness,\n",
    "        cv2.LINE_AA\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_yolo_labels(image_path, label_path):\n",
    "    \"\"\"\n",
    "    Función destinada a la visualización de las mamografías destacando la localización de los tumores existentes  \n",
    "    simulando lo mejor posible el estilo propio del output de YOLO. \n",
    "    \"\"\"\n",
    "    # Lee la imagen .png\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(\"No se pudo cargar la imagen:\", image_path)\n",
    "        return\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Lee el archivo .txt con las etiquetas YOLO asociado a la imagen tomada anteriormente\n",
    "    try:\n",
    "        with open(label_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "    except:\n",
    "        print(\"No se pudo leer el archivo de etiquetas:\", label_path)\n",
    "        return\n",
    "\n",
    "    # Obtiene la información procedente de las etiquetas cuyo formato es: (class_id x_center y_center w_norm h_norm)\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 5:\n",
    "            continue\n",
    "\n",
    "        class_id, x_center_norm, y_center_norm, w_norm, h_norm = parts\n",
    "        x_center_norm = float(x_center_norm)\n",
    "        y_center_norm = float(y_center_norm)\n",
    "        w_box_norm = float(w_norm)\n",
    "        h_box_norm = float(h_norm)\n",
    "\n",
    "        # Convierte a píxeles reales\n",
    "        x_center = x_center_norm * w\n",
    "        y_center = y_center_norm * h\n",
    "        w_box = w_box_norm * w\n",
    "        h_box = h_box_norm * h\n",
    "\n",
    "        x1 = x_center - (w_box / 2)\n",
    "        y1 = y_center - (h_box / 2)\n",
    "        x2 = x_center + (w_box / 2)\n",
    "        y2 = y_center + (h_box / 2)\n",
    "\n",
    "        # Dibuja los rectángulos propios eel estilo YOLO\n",
    "        draw_box_yolo_style(\n",
    "            img, \n",
    "            x1, y1, x2, y2, \n",
    "            label=\"tumor\", \n",
    "            score=1.0,\n",
    "            font_scale=3,\n",
    "            font_thickness=14\n",
    "        )\n",
    "\n",
    "    # Visualiza los resultados\n",
    "    import matplotlib.pyplot as plt\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de las etiquetas predichas por del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_predict(image_path):\n",
    "    \"\"\"\n",
    "    Función destinada a la visualización de las mamografías destacando la localización de los tumores existentes \n",
    "    según las predicciones obtenidas por YOLO.\n",
    "    \"\"\"\n",
    "    # Carga el modelo entrenado\n",
    "    model = YOLO(\"runs/detect/mammo_yolo2/weights/best.pt\")\n",
    "\n",
    "    # Predecice la posición de los tumores con YOLO\n",
    "    results = model.predict(source=image_path)\n",
    "\n",
    "    # Obtiene el resultado\n",
    "    res = results[0]\n",
    "\n",
    "    # Dibujan las cajas en la imagen\n",
    "    img_annotated = res.plot()\n",
    "\n",
    "    # Muestra el resultado\n",
    "    plt.imshow(img_annotated)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación gráfica entre las etiquetas reales y las predichas por el modelo YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\train\\Mass_Training_P_00039_RIGHT_CC.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\train\\Mass_Training_P_00039_RIGHT_CC.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\train\\Mass_Training_P_00914_LEFT_CC.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\train\\Mass_Training_P_00914_LEFT_CC.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\train\\Mass_Training_P_00148_RIGHT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\train\\Mass_Training_P_00148_RIGHT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\train\\Mass_Training_P_01529_RIGHT_CC.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\train\\Mass_Training_P_01529_RIGHT_CC.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\train\\Mass_Training_P_01083_RIGHT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\train\\Mass_Training_P_01083_RIGHT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_00200_RIGHT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_00200_RIGHT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_00677_RIGHT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_00677_RIGHT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_00773_LEFT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_00773_LEFT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_01800_LEFT_CC.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_01800_LEFT_CC.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_00662_LEFT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_00662_LEFT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_00629_RIGHT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_00629_RIGHT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_00099_LEFT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_00099_LEFT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_00126_RIGHT_CC.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_00126_RIGHT_CC.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_00758_LEFT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_00758_LEFT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_01912_RIGHT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_01912_RIGHT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_01578_LEFT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_01578_LEFT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_01323_LEFT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_01323_LEFT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_01251_LEFT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_01251_LEFT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_01235_RIGHT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_01235_RIGHT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_00591_RIGHT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_00591_RIGHT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "image_path = r\"C:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\images\\val\\Mass_Test_P_00391_RIGHT_MLO.png\"\n",
    "label_path = r\"c:\\Users\\mcamp\\OneDrive\\Documentos\\TFG_P2\\manifest-ZkhPvrLo5216730872708713142\\CBIS-DDSM\\YOLO\\labels\\val\\Mass_Test_P_00391_RIGHT_MLO.txt\"\n",
    "draw_yolo_labels(image_path, label_path)\n",
    "yolo_predict(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
